{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaumyaN18/PROJECT-49/blob/main/Copy_of_C_128_CNL_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVe_ge-0KTVn"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "**Image Dataset Directory Structur**e:\n",
        "\n",
        "**Note: The directory and sub-directory names shown here are only for explanation purposes which might differ from the code.**\n",
        "\n",
        "Suppose if we have a master directory(folder) of the Images then we can subdivide it into “Training”, “Validation” & “Testing” images sub-directories(sub-folder).\n",
        "\n",
        "And then the “Training” directories contain sub-directories(sub-folders) called “Infected” and “Uninfected” which contain appropriate images in the respective sub-directories.\n",
        "\n",
        "Similarly, the “Validation'' & “Testing” directory also contains sub-directories(sub-folders) called “Infected” and “Uninfected” which contain appropriate images in the respective sub-directories.\n",
        "\n",
        "\n",
        "**Training**: Images in this directory will be used for the training of the data.\n",
        "\n",
        "**Validation**: Images in this directory will be used to validate the model training. The validation dataset allows us to see how well the data generalises the classification.\n",
        "\n",
        "**Testing**: Images in this directory will be used to test how well the model is trained.\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/2467514a-e93f-4a0f-8e20-b3893dfa9144.jpeg\" width= 600>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xCYT8gXKTVq",
        "outputId": "84d01fdf-b3a1-4938-d47d-17c87d1600a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PRO-M3-Pneumothorax-Image-Dataset'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Counting objects: 100% (300/300), done.\u001b[K\n",
            "remote: Compressing objects: 100% (297/297), done.\u001b[K\n",
            "remote: Total 313 (delta 3), reused 300 (delta 3), pack-reused 13\u001b[K\n",
            "Receiving objects: 100% (313/313), 118.60 MiB | 32.39 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (602/602), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/procodingclass/PRO-M3-Pneumothorax-Image-Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0nG4OkcN26G"
      },
      "source": [
        "## Image Preprocessing\n",
        "\n",
        "1. Convert each image to an array\n",
        "2. Map each image labels\n",
        "3. Augment the each image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtXxDq1O4okc"
      },
      "source": [
        "### Image Preprocessing: Mapping each image with labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DIAsjUd343_"
      },
      "source": [
        "<center><b>Mapping Each Image With Labels</b><br><img src=\"https://s3-whjr-curriculum-uploads.whjr.online/1bdade80-2a32-4fc2-8902-f18067803dba.jpeg\" width= 1000>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6cHrlB-5xQ4"
      },
      "source": [
        "### Image Preprocessing: Data Augmentation\n",
        "\n",
        "A few Data Augemtation Techniques:\n",
        "\n",
        "*   Image Rotation\n",
        "*   Image Height & Width Shift\n",
        "*   Image Horizontal & Vertical Flipping\n",
        "*   Image Resizing\n",
        "*   Image Zooming\n",
        "\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/5403e9b1-a339-405d-98b3-36826ec3f04a.gif\" width= 400>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NhJWjpUVBpu"
      },
      "source": [
        "#### Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5-_nrO7yn2D",
        "outputId": "5375fe60-1ef4-4346-c061-bf58ec39e2fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 200 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "\n",
        "# Random Data Augmentation(Rescale, Rotation, Flips, Zoom, Shifts) using ImageDataGenerator\n",
        "training_data_generator = ImageDataGenerator(\n",
        "    rescale = 1.0/255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "\n",
        "# Image Directory\n",
        "training_image_directory = \"/content/PRO-M3-Pneumothorax-Image-Dataset/training_dataset\"\n",
        "\n",
        "# Generate Preprocessed Augmented Data\n",
        "training_augmented_images = training_data_generator.flow_from_directory(\n",
        "    training_image_directory,\n",
        "    target_size=(180,180))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUOaQFdFVGmT"
      },
      "source": [
        "#### Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS-Jx1EB-OFg",
        "outputId": "0954ee38-fb0b-47ab-e16b-7c6c8fd2d676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 200 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Random Data Augmentation(Rescale) using ImageDataGenerator\n",
        "validation_data_generator = ImageDataGenerator(rescale = 1.0/255)\n",
        "\n",
        "# Image Directory\n",
        "validation_image_directory = \"/content/PRO-M3-Pneumothorax-Image-Dataset/validation_dataset\"\n",
        "\n",
        "# Generate Preprocessed Augmented Data\n",
        "validation_augmented_images = validation_data_generator.flow_from_directory(\n",
        "    validation_image_directory,\n",
        "    target_size=(180,180))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TFZxDyWZjRb"
      },
      "source": [
        "#### Class Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUU7UKNhyftg",
        "outputId": "cb01b8de-091b-4b86-c6f6-3fd1b4235931"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'infected': 0, 'uninfected': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "training_augmented_images.class_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKguVEnx3O4m"
      },
      "source": [
        "## Convolutional Neural Network Architecture\n",
        "A CNN model have:\n",
        "\n",
        "1. **Feature Learning layers**:\n",
        "\n",
        "   1.1 Convolution + Activation(RELU) layers\n",
        "\n",
        "   1.2 Pooling layers\n",
        "\n",
        "2. **Classification layers**:\n",
        "\n",
        "   2.1 Flatten layer\n",
        "\n",
        "   2.2 Fully connected(Dense) layer\n",
        "\n",
        "   2.3 Fully connected(Dense) layer with Softmax\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/9d09af47-3a9d-48b7-a05c-8941009442ea.png\" width= 1500>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T-8YEZK3aj_"
      },
      "source": [
        "**Feature Extraction Visualisation(Convolution + Relu)**\n",
        "\n",
        "The convolution is a mathematical computation between two arrays, the image array and the filter array which gives a new image array.\n",
        "\n",
        "\n",
        "Visually we can understand that the feature detector/filter moves over the image to extract features from the image.\n",
        "\n",
        "\n",
        "\n",
        "[<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/3917c089-1d8f-4f32-b5c4-401b6abe8d47.gif\" width= 500>](https://)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHHUNrcWC91w"
      },
      "source": [
        "## Mathematically:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tQ9L7AzCXN3"
      },
      "source": [
        "**Conv2D Layer**\n",
        "\n",
        "The convolution is a mathematical computation between two 2D arrays, the image array and the filter array which gives a new image array.\n",
        "\n",
        "A portion of the input image array matrix, called a sub-array(size same as the size of the filter) is taken, starting from the top left.\n",
        "\n",
        "This sub-array is multiplied with the filter array. We can multiply one array matrix to another, by multiplying 1st element to 1st element of both the arrays(2nd element to 2nd element of both arrays and so on).\n",
        "\n",
        "After multiplying the result is added, which gives the value of the 1st element of the new image array.\n",
        "\n",
        "Then we shift towards right by one column, repeat the above steps to get the value of the 2nd element of the new array.\n",
        "\n",
        "Once the whole row is finished we shift downwards by one row, repeat the above steps to get the value of all elements of the new array one by one.\n",
        "\n",
        "The whole process is repeated with different filters, to get different output, which all together is the output of the 1st Conv2D layer.\n",
        "\n",
        "These outputs from the 1st Conv2D layer are given to the 2nd Conv2D layer and convolutions are performed.\n",
        "\n",
        "This repeated for all the layers of the CNN model.\n",
        "\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/306591ba-2163-45d9-9c60-dbc895332982.gif\" width= 800>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZhK3L6qCW0C"
      },
      "source": [
        "**ReLU**\n",
        "\n",
        "ReLU is defined as a function, y= f(x) such that it gives x for all values of x > 0 and 0 for all values of x<0.\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/6986c680-7ad3-4fad-bf04-59cecfa5e0e3.png\" width= 600>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yV1sDPWBZsS"
      },
      "source": [
        "**MaxPooling2D**\n",
        "\n",
        "First there is an input array(for example 4x4) and pool size(for example 2x2). Pool size is always smaller than the input array size.\n",
        "\n",
        "Then the maximum value is taken from the sub-array of size equal to pool size.\n",
        "\n",
        "The result after applying the Max Pooling will be the new array of size equal to the half of the size of the original input array.\n",
        "\n",
        "Since our input array is 4x4, after max pooling the new array will be 2x2, hence reducing the dimension of the array.\n",
        "\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/d485cb40-4db6-4fb6-9051-d9e888883053.jpg\" width= 800>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hMRvgb4W7MF"
      },
      "source": [
        "## Define Convolution Neural Network(CNN) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVT96uWaZUtF"
      },
      "outputs": [],
      "source": [
        "from keras.src.layers.serialization import activation\n",
        "import tensorflow as tf\n",
        "\n",
        "## ADD CODE HERE\n",
        "model = tf.keras.models.Sequential([\n",
        "    #first convutional and pooling layer\n",
        "    tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(180,180,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #second convutional and pooling layer\n",
        "    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Third convutional and Pooling\n",
        "    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Fourth Convutional and Polling Layers\n",
        "    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Flaten The Result before passing to dense layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "    #Classification Layers\n",
        "    tf.keras.layers.Dense(512,activation='relu'),\n",
        "    tf.keras.layers.Dense(2,activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuhJ_4N6XCle"
      },
      "source": [
        "## Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOj029YuZXRA",
        "outputId": "1bd7cc42-5885-4551-aa8c-159ce1e22515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 178, 178, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 89, 89, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 87, 87, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 43, 43, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 41, 41, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 20, 20, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 18, 18, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 9, 9, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 5184)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5184)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               2654720   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2768322 (10.56 MB)\n",
            "Trainable params: 2768322 (10.56 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "## ADD CODE HERE\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIvxXyGiwRAH",
        "outputId": "2f588ccd-51bf-4f2b-a83d-9bc706f0fa12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 33s 4s/step - loss: 0.7015 - accuracy: 0.4550 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 31s 4s/step - loss: 0.6961 - accuracy: 0.4700 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 34s 5s/step - loss: 0.6963 - accuracy: 0.4600 - val_loss: 0.6929 - val_accuracy: 0.4900\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 30s 4s/step - loss: 0.6939 - accuracy: 0.4700 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 30s 4s/step - loss: 0.6938 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 34s 5s/step - loss: 0.6941 - accuracy: 0.4900 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 30s 4s/step - loss: 0.7015 - accuracy: 0.4900 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 34s 5s/step - loss: 0.6938 - accuracy: 0.4600 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 30s 4s/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 30s 4s/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = [\"accuracy\"])\n",
        "history = model.fit(training_augmented_images,epochs = 10,validation_data = validation_augmented_images,verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7S3M1b715-T"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxGDUSMw5Epv",
        "outputId": "601e8fb9-a114-46c2-c59f-f9a79e0ee5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(\"pneumothorax.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.image import imread\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Testing image directory\n",
        "testing_image_directory = '/content/PRO-M3-Pneumothorax-Image-Dataset/testing_dataset/infected'\n",
        "\n",
        "# All image files in the directory\n",
        "img_files = os.listdir(testing_image_directory)\n",
        "\n",
        "i= 0\n",
        "\n",
        "# Loop through an 9 image files\n",
        "for file in img_files[51:60]:\n",
        "\n",
        "  # full path of the image\n",
        "  img_files_path = os.path.join(testing_image_directory, file)\n",
        "\n",
        "  # load image\n",
        "  img_1 = load_img(img_files_path,target_size=(180, 180))\n",
        "\n",
        "  # convert image to an array\n",
        "  img_2 = img_to_array(img_1)\n",
        "\n",
        "  # increase the dimension\n",
        "  img_3 = np.expand_dims(img_2, axis=0)\n",
        "\n",
        "  # predict the class of an unseen image\n",
        "  prediction = model.predict(img_3)\n",
        "  print(prediction)\n",
        "\n",
        "\n",
        "  predict_class = np.argmax(prediction, axis=0)\n",
        "  # print(predict_class)\n",
        "\n",
        "  # plot the image using subplot\n",
        "  pyplot.subplot(3, 3, i+1)\n",
        "  pyplot.imshow(img_2.astype('uint8'))\n",
        "\n",
        "  # Add title of the plot as predicted class value\n",
        "  pyplot.title(predict_class[0])\n",
        "\n",
        "  # Do not show x and y axis with the image\n",
        "  pyplot.axis('off')\n",
        "\n",
        "  i=i+1\n",
        "\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "l2PN9Tdp8Hkw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "195ce29b-9766-461c-bcc6-bac62212e588"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-633704e4346a>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# All image files in the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mimg_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_image_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/PRO-M3-Pneumothorax-Image-Dataset/testing_dataset/infected'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# print(acc)\n",
        "# print(val_acc)\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "pyplot.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "pyplot.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "\n",
        "pyplot.title('Training and validation accuracy')\n",
        "\n",
        "pyplot.legend()\n",
        "\n",
        "pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "n7BtU5eOxwQ8",
        "outputId": "66c6e5ec-f665-498c-f3d8-ba4c2753bb15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6a645ae4838e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}